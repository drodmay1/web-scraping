{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Web Scraping and NLP with Requests, BeautifulSoup, and spaCy\n",
    "\n",
    "### Student Name: David Rodriguez-Mayorquin\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "annotated-types           0.7.0\n",
      "anyio                     4.6.2.post1\n",
      "appnope                   0.1.4\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     24.2.0\n",
      "babel                     2.16.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.2.0\n",
      "blis                      1.0.1\n",
      "catalogue                 2.0.10\n",
      "certifi                   2024.8.30\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.0\n",
      "click                     8.1.7\n",
      "cloudpathlib              0.20.0\n",
      "comm                      0.2.2\n",
      "confection                0.1.5\n",
      "contourpy                 1.3.1\n",
      "cycler                    0.12.1\n",
      "cymem                     2.0.8\n",
      "debugpy                   1.8.8\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "executing                 2.1.0\n",
      "fastjsonschema            2.20.0\n",
      "fonttools                 4.54.1\n",
      "fqdn                      1.5.1\n",
      "h11                       0.14.0\n",
      "html5lib                  1.1\n",
      "httpcore                  1.0.6\n",
      "httpx                     0.27.2\n",
      "idna                      3.10\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.29.0\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.4\n",
      "joblib                    1.4.2\n",
      "json5                     0.9.28\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.2\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.3.0\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "kiwisolver                1.4.7\n",
      "langcodes                 3.4.1\n",
      "language_data             1.2.0\n",
      "marisa-trie               1.2.1\n",
      "markdown-it-py            3.0.0\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib                3.9.2\n",
      "matplotlib-inline         0.1.7\n",
      "mdurl                     0.1.2\n",
      "mistune                   3.0.2\n",
      "murmurhash                1.0.10\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "nltk                      3.9.1\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.0.2\n",
      "overrides                 7.7.0\n",
      "packaging                 24.2\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pillow                    11.0.0\n",
      "pip                       24.3.1\n",
      "platformdirs              4.3.6\n",
      "preshed                   3.0.9\n",
      "prometheus_client         0.21.0\n",
      "prompt_toolkit            3.0.48\n",
      "psutil                    6.1.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pycparser                 2.22\n",
      "pydantic                  2.9.2\n",
      "pydantic_core             2.23.4\n",
      "Pygments                  2.18.0\n",
      "pyparsing                 3.2.0\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.0\n",
      "referencing               0.35.1\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rich                      13.9.4\n",
      "rpds-py                   0.21.0\n",
      "Send2Trash                1.8.3\n",
      "setuptools                75.4.0\n",
      "shellingham               1.5.4\n",
      "six                       1.16.0\n",
      "smart-open                7.0.5\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "spacy                     3.8.2\n",
      "spacy-legacy              3.0.12\n",
      "spacy-loggers             1.0.5\n",
      "spacytextblob             5.0.0\n",
      "srsly                     2.4.8\n",
      "stack-data                0.6.3\n",
      "terminado                 0.18.1\n",
      "textblob                  0.18.0.post0\n",
      "thinc                     8.3.2\n",
      "tinycss2                  1.4.0\n",
      "tornado                   6.4.1\n",
      "tqdm                      4.67.0\n",
      "traitlets                 5.14.3\n",
      "typer                     0.13.0\n",
      "types-python-dateutil     2.9.0.20241003\n",
      "typing_extensions         4.12.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.3\n",
      "wasabi                    1.1.3\n",
      "wcwidth                   0.2.13\n",
      "weasel                    0.4.1\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "wrapt                     1.16.0\n",
      "All prereqs installed.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "import requests\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip list\n",
    "\n",
    "print('All prereqs installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1. \n",
    "\n",
    "Write code that extracts the article html from https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/ and dumps it to a .pkl (or other appropriate file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved to article.pkl\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pickle\n",
    "\n",
    "# URL of the article\n",
    "url = \"https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/\"\n",
    "\n",
    "# request to get the HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# determine pkl file name\n",
    "file_name = \"article.pkl\"\n",
    "\n",
    "# Save HTML content to a pickle file\n",
    "with open(file_name, \"wb\") as file:\n",
    "    pickle.dump(html_content, file)\n",
    "\n",
    "print(f\"HTML content saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. \n",
    "\n",
    "Read in your article's html source from the file you created in question 1 and print it's text (use `.get_text()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How Laser Headlights Work | Hackaday\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hackaday\n",
      "\n",
      "\n",
      "Primary Menu\n",
      "\n",
      "Home\n",
      "Blog\n",
      "Hackaday.io\n",
      "Tindie\n",
      "Hackaday Prize\n",
      "Submit\n",
      "About\n",
      "\n",
      "\n",
      "Search for:\n",
      "\n",
      "\n",
      "\n",
      " March 27, 2021 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How Laser Headlights Work\n",
      "\n",
      "\n",
      "                130 Comments            \n",
      "\n",
      "by:\n",
      "Lewin Day\n",
      "\n",
      "\n",
      "\n",
      "March 22, 2021\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "When we think about the onward march of automotive technology, headlights aren’t usually the first thing that come to mind. Engines, fuel efficiency, and the switch to electric power are all more front of mind. However, that doesn’t mean there aren’t thousands of engineers around the world working to improve the state of the art in automotive lighting day in, day out.\n",
      "Sealed beam headlights gave way to more modern designs once regulations loosened up, while bulbs moved from simple halogens to xenon HIDs and, more recently, LEDs. Now, a new technology is on the scene, with lasers!\n",
      "\n",
      "Laser Headlights?!\n",
      "BWM’s prototype\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load HTML content from article.pkl\n",
    "file_name = \"article.pkl\"\n",
    "with open(file_name, \"rb\") as file:\n",
    "    html_content = pickle.load(file)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Extract and print the text\n",
    "article_text = soup.get_text()\n",
    "print(article_text[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3. \n",
    "\n",
    "Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most frequent tokens are:\n",
      "Token: 'laser', Frequency: 30\n",
      "Token: 'headlights', Frequency: 17\n",
      "Token: 'technology', Frequency: 10\n",
      "Token: 'headlight', Frequency: 10\n",
      "Token: 'led', Frequency: 10\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the article text\n",
    "file_name = \"article.pkl\"\n",
    "with open(file_name, \"rb\") as file:\n",
    "    html_content = pickle.load(file)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "article_text = soup.get_text()\n",
    "\n",
    "# Load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Locate main content\n",
    "main_content = soup.find(\"div\", class_=\"entry-content\")\n",
    "article_text = main_content.get_text() if main_content else \"\"\n",
    "\n",
    "# Process text with spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Filtered tokens\n",
    "tokens = [\n",
    "    token.text.lower() for token in doc\n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "]\n",
    "\n",
    "# Count token frequencies\n",
    "token_freq = Counter(tokens)\n",
    "\n",
    "# Get the 5 most frequent tokens\n",
    "most_common_tokens = token_freq.most_common(5)\n",
    "\n",
    "# Print most frequent tokens\n",
    "print(\"The 5 most frequent tokens are:\")\n",
    "for token, freq in most_common_tokens:\n",
    "    print(f\"Token: '{token}', Frequency: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define the following methods:\n",
    "    * `score_sentence_by_token(sentence, interesting_token)` that takes a sentence and a list of interesting token and returns the number of times that any of the interesting words appear in the sentence divided by the number of words in the sentence\n",
    "    * `score_sentence_by_lemma(sentence, interesting_lemmas)` that takes a sentence and a list of interesting lemmas and returns the number of times that any of the interesting lemmas appear in the sentence divided by the number of words in the sentence\n",
    "    \n",
    "You may find some of the code from the in class notes useful; feel free to use methods (rewrite them in this cell as well).  Test them by showing the score of the first sentence in your article using the frequent tokens and frequent lemmas identified in question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Which tokens and lexems would be ommitted from the lists generated in questions 3 and 4 if we only wanted to consider nouns as interesting words?  How might we change the code to only consider nouns? Put your answer in this Markdown cell (you can edit it by double clicking it)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
